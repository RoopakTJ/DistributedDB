{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Group Assignment - Part C - Spark Straming Application\n",
    "\n",
    "**Your Details:**\n",
    "- Name: Roopak Thiyyathuparambil Jayachandran\n",
    "- StudentID: 29567467\n",
    "- Email: rthi0002@student.monash.edu\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Processes the data which is pushed by Kafka producers into the stream. Data is joined and pushed to mongodb \n",
    "Methodology:\n",
    "* Creating spark connection for the stream\n",
    "* Understanding the producer from which data has come and saving it to appropriate table in memory\n",
    "* Joining the data based on geo hash value and pushing it to mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology for implementation:\n",
    "\n",
    "    * The data is taken into a list and final element is checked to identify the producer.\n",
    "    * Hash value of the location is stored irrespective of producer.\n",
    "    * If data is coming from producer 1, extract the required fields, store to a dictionary and check if the same location data at almost same time is present in hotspot data. If present get that data and return the complete dictionary. Else return the climate data without hotspot data.\n",
    "    * If data is coming from Aqua or Terra Satellite, check if the similar location is already stored in the list. If present store the average of confidence and surface temperature (as per requirement). Else extract the data to a list, iterate the climate data and check matching hash value, hour and minute value (seconds are avoided) and return the climate dictionary\n",
    "    * The dictionary is then stored to the mongodb using insert function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating global list of climate and hotspot. This will be used to store dictionaries of climate and hotspot.\n",
    "climate = []\n",
    "hotspot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pandas as pd\n",
    "import geohash as gh\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "global climate\n",
    "global hotspot\n",
    "\n",
    "# Function which does the extraction from stream data and returning a dictionary\n",
    "def sep(input_list):\n",
    "    latitude = float(input_list[0])\n",
    "    longitude = float(input_list[0])\n",
    "    \n",
    "    geo = gh.encode(float(input_list[0]),float(input_list[1]), precision = 5)\n",
    "    if(int(input_list[-1]) == 1): # Producer 1\n",
    "        # Extracting the date\n",
    "        date = input_list[7].split(\" \")[0]\n",
    "        # Extracting the hour \n",
    "        hour = int(input_list[7].split(\" \")[1].split(\":\")[0])\n",
    "        climate_dic = {}\n",
    "        climate_dic[\"geo\"] = geo\n",
    "        climate_dic[\"air_temperature\"] = int(input_list[2])\n",
    "        climate_dic[\"relative_humidity\"] = float(input_list[3])\n",
    "        climate_dic[\"windspeed_knots\"] = float(input_list[4])\n",
    "        climate_dic[\"windspeed_max\"] = float(input_list[5])\n",
    "        climate_dic[\"precipitation\"] = input_list[6]\n",
    "        climate_dic[\"date\"] = date\n",
    "        climate_dic[\"hour\"] = hour  \n",
    "        \n",
    "        fire_present = False # Flag check if data is already stored from the other satelite data\n",
    "        if len(hotspot) > 1: \n",
    "            for each in hotspot:\n",
    "                if (each[\"geo\"] == geo and each[\"date\"] == date and each[\"hour\"] == hour):\n",
    "                    climate_dic[\"historic\"] = [each]\n",
    "                    fire_present = True        \n",
    "                    break\n",
    "            if not fire_present:\n",
    "                each[\"historic\"] = []\n",
    "        \n",
    "        climate.append(climate_dic)\n",
    "        return climate_dic\n",
    "        \n",
    "        \n",
    "    else: # Producer 2\n",
    "        date = input_list[4].split(\" \")[0] # Extract date\n",
    "        hour = int(input_list[4].split(\" \")[1].split(\":\")[0]) # Extract hour\n",
    "        minute = int(input_list[4].split(\" \")[1].split(\":\")[1]) # Extract minute\n",
    "        present = False\n",
    "        hotspot_dic = {}\n",
    "        for each in hotspot: # Iterating through the hotspot data and check if simiar data from other satelite is already stored\n",
    "            if (each[\"geo\"] == geo and each[\"hour\"] == hour and each[\"minute\"] == minute):\n",
    "                each[\"confidence\"] = (each[\"confidence\"] + int(input_list[2]))/2\n",
    "                each[\"surface_temperature_celsius\"] = (each[\"surface_temperature_celsius\"] + int(input_list[3]))/2\n",
    "                for each2 in climate:\n",
    "                    if (each2[\"geo\"] == geo and each2[\"date\"] == date and each2[\"hour\"] == hour):   \n",
    "                        each2[\"historic\"] = each\n",
    "                        return each2\n",
    "                present = True\n",
    "                break\n",
    "        if not present: # Not present, create a hotspot dictionary and \n",
    "            hotspot_dic[\"geo\"] = geo\n",
    "            hotspot_dic[\"date\"] = date\n",
    "            hotspot_dic[\"latitude\"] = float(input_list[0])\n",
    "            hotspot_dic[\"longitude\"] = float(input_list[1])\n",
    "            hotspot_dic[\"confidence\"] = int(input_list[2])\n",
    "            hotspot_dic[\"surface_temperature_celsius\"] = int(input_list[3])\n",
    "            hotspot_dic[\"datetime\"] = input_list[4]\n",
    "            hotspot_dic[\"hour\"] = hour\n",
    "            hotspot_dic[\"minute\"] = minute\n",
    "            for each in climate:\n",
    "                if (each[\"geo\"] == geo and each[\"date\"] == date and each[\"hour\"] == hour):\n",
    "                    each[\"historic\"] = hotspot_dic\n",
    "                    return each      \n",
    "                    break\n",
    "            hotspot.append(hotspot_dic)\n",
    "            return {}\n",
    "            \n",
    "       \n",
    "def sendDataToDB(iter): # Initializes the MongoDB connection\n",
    "    client = MongoClient()\n",
    "    db = client.fit514\n",
    "    col = db.climate_data_model_9\n",
    "    for record in iter:\n",
    "        # Splitting the incoming stream of data to a list\n",
    "        data = record[1].split(\",\")\n",
    "        dic = {}\n",
    "        dic = sep(data)\n",
    "        try:\n",
    "            if len(dic) == 0: \n",
    "                continue\n",
    "            print(dic)\n",
    "            dic['_id'] = ObjectId()\n",
    "            col.replace_one({\"_id\":dic[\"_id\"]}, dic, True)\n",
    "            print(\"Inserted: \", col.count() )\n",
    "            print()\n",
    "            print()\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "n_secs = 2\n",
    "topic = \"Climate\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "print(\"1\")\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'assign-climate', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "print(\"2\")\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is helpful for debugging logic implemented in sep function. Only for development Purpose. Can be used to verify how each of the requirement is satisfied and what value is returned. Can be also used to verify edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash as gh\n",
    "def sep(input_list):\n",
    "    latitude = float(input_list[0])\n",
    "    longitude = float(input_list[0])\n",
    "    \n",
    "    geo = gh.encode(float(input_list[0]),float(input_list[1]), precision = 5)\n",
    "    if(int(input_list[-1]) == 1):\n",
    "        date = input_list[7].split(\" \")[0]\n",
    "        hour = int(input_list[7].split(\" \")[1].split(\":\")[0])\n",
    "        climate_dic = {}\n",
    "        climate_dic[\"geo\"] = geo\n",
    "        climate_dic[\"air_temperature\"] = int(input_list[2])\n",
    "        climate_dic[\"relative_humidity\"] = float(input_list[3])\n",
    "        climate_dic[\"windspeed_knots\"] = float(input_list[4])\n",
    "        climate_dic[\"windspeed_max\"] = float(input_list[5])\n",
    "        climate_dic[\"precipitation\"] = input_list[6]\n",
    "        climate_dic[\"date\"] = date\n",
    "        climate_dic[\"hour\"] = hour  \n",
    "        \n",
    "        fire_present = False\n",
    "        if len(hotspot) > 1:\n",
    "            for each in hotspot:\n",
    "                if (each[\"geo\"] == geo and each[\"date\"] == date and each[\"hour\"] == hour):\n",
    "                    climate_dic[\"historic\"] = [each]\n",
    "                    fire_present = True        \n",
    "                    break\n",
    "            if not fire_present:\n",
    "                each[\"historic\"] = []\n",
    "        \n",
    "        climate.append(climate_dic)\n",
    "        return climate_dic\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        date = input_list[4].split(\" \")[0]\n",
    "        hour = int(input_list[4].split(\" \")[1].split(\":\")[0])\n",
    "        minute = int(input_list[4].split(\" \")[1].split(\":\")[1])\n",
    "        present = False\n",
    "        hotspot_dic = {}\n",
    "        for each in hotspot:\n",
    "            if (each[\"geo\"] == geo and each[\"hour\"] == hour and each[\"minute\"] == minute):\n",
    "                each[\"confidence\"] = (each[\"confidence\"] + int(input_list[2]))/2\n",
    "                each[\"surface_temperature_celsius\"] = (each[\"surface_temperature_celsius\"] + int(input_list[3]))/2\n",
    "                for each2 in climate:\n",
    "                    if (each2[\"geo\"] == geo and each2[\"date\"] == date and each2[\"hour\"] == hour):   \n",
    "                        each2[\"historic\"] = each\n",
    "                        return each2\n",
    "                present = True\n",
    "                break\n",
    "        if not present:\n",
    "            hotspot_dic[\"geo\"] = geo\n",
    "            hotspot_dic[\"date\"] = date\n",
    "            hotspot_dic[\"latitude\"] = float(input_list[0])\n",
    "            hotspot_dic[\"longitude\"] = float(input_list[1])\n",
    "            hotspot_dic[\"confidence\"] = int(input_list[2])\n",
    "            hotspot_dic[\"surface_temperature_celsius\"] = int(input_list[3])\n",
    "            hotspot_dic[\"datetime\"] = input_list[4]\n",
    "            hotspot_dic[\"hour\"] = hour\n",
    "            hotspot_dic[\"minute\"] = minute\n",
    "            for each in climate:\n",
    "                if (each[\"geo\"] == geo and each[\"date\"] == date and each[\"hour\"] == hour):\n",
    "                    each[\"historic\"] = hotspot_dic\n",
    "                    return each      \n",
    "                    break\n",
    "            \n",
    "            hotspot.append(hotspot_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': 72,\n",
       "  'date': '2019-05-25',\n",
       "  'datetime': '2019-05-25 19:13:03.746296',\n",
       "  'geo': 'r1w9j',\n",
       "  'hour': 19,\n",
       "  'latitude': -36.3634,\n",
       "  'longitude': 144.3871,\n",
       "  'minute': 13,\n",
       "  'surface_temperature_celsius': 47},\n",
       " {'confidence': 72,\n",
       "  'date': '2019-05-25',\n",
       "  'datetime': '2019-05-25 19:13:03.746296',\n",
       "  'geo': 'r3815',\n",
       "  'hour': 19,\n",
       "  'latitude': -36.3634,\n",
       "  'longitude': 146.3871,\n",
       "  'minute': 13,\n",
       "  'surface_temperature_celsius': 47},\n",
       " {'confidence': 72,\n",
       "  'date': '2019-05-25',\n",
       "  'datetime': '2019-05-25 19:13:03.746296',\n",
       "  'geo': 'r60pg',\n",
       "  'historic': [],\n",
       "  'hour': 19,\n",
       "  'latitude': -32.3634,\n",
       "  'longitude': 146.3871,\n",
       "  'minute': 13,\n",
       "  'surface_temperature_celsius': 47}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_list = \"-36.3634,146.3871,72,47,2019-05-25 19:13:03.746296,2\"\n",
    "input_list = \"-36.3634,146.3871,15,41.2,13.8,16.9, 0.00G,2019-05-25 19:13:03.746298,1\"\n",
    "sep(input_list=input_list.split(\",\"))\n",
    "hotspot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'air_temperature': 15,\n",
       "  'date': '2019-05-25',\n",
       "  'geo': 'r3374',\n",
       "  'hour': 18,\n",
       "  'precipitation': ' 0.00G',\n",
       "  'relative_humidity': 41.2,\n",
       "  'windspeed_knots': 13.8,\n",
       "  'windspeed_max': 16.9},\n",
       " {'air_temperature': 15,\n",
       "  'date': '2019-05-25',\n",
       "  'geo': 'r3815',\n",
       "  'historic': [{'confidence': 72,\n",
       "    'date': '2019-05-25',\n",
       "    'datetime': '2019-05-25 19:13:03.746296',\n",
       "    'geo': 'r3815',\n",
       "    'hour': 19,\n",
       "    'latitude': -36.3634,\n",
       "    'longitude': 146.3871,\n",
       "    'minute': 13,\n",
       "    'surface_temperature_celsius': 47}],\n",
       "  'hour': 19,\n",
       "  'precipitation': ' 0.00G',\n",
       "  'relative_humidity': 41.2,\n",
       "  'windspeed_knots': 13.8,\n",
       "  'windspeed_max': 16.9}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
